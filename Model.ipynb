{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'xo_differ'\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(x, keep_prob, y_, output_node_name):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    # 28*28*1\n",
    "\n",
    "    conv1 = tf.layers.conv2d(x_image, 64, 3, 1, 'same', activation=tf.nn.relu)\n",
    "    # 28*28*64\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, 2, 2, 'same')\n",
    "    # 14*14*64\n",
    "\n",
    "    conv2 = tf.layers.conv2d(pool1, 128, 3, 1, 'same', activation=tf.nn.relu)\n",
    "    # 14*14*128\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, 2, 2, 'same')\n",
    "    # 7*7*128\n",
    "\n",
    "    conv3 = tf.layers.conv2d(pool2, 256, 3, 1, 'same', activation=tf.nn.relu)\n",
    "    # 7*7*256\n",
    "    pool3 = tf.layers.max_pooling2d(conv3, 2, 2, 'same')\n",
    "    # 4*4*256\n",
    "\n",
    "    flatten = tf.reshape(pool3, [-1, 4*4*256])\n",
    "    fc = tf.layers.dense(flatten, 1024, activation=tf.nn.relu)\n",
    "    dropout = tf.nn.dropout(fc, keep_prob)\n",
    "    logits = tf.layers.dense(dropout, 2)\n",
    "    outputs = tf.nn.softmax(logits, name=output_node_name)\n",
    "\n",
    "    # loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits))\n",
    "\n",
    "    # train step\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(outputs, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    return train_step, loss, accuracy, merged_summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_node_name = 'input'\n",
    "keep_prob_node_name = 'keep_prob'\n",
    "output_node_name = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28*28], name=input_node_name)\n",
    "keep_prob = tf.placeholder(tf.float32, name=keep_prob_node_name)\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    n1 = np.load(\"O.npy\")\n",
    "    n2 = np.load(\"X.npy\")\n",
    "    print(n2.shape)\n",
    "    print(n1.shape)\n",
    "    x = n1 \n",
    "    x = np.concatenate((x,n2),axis=0)\n",
    "    length1 = n1.shape[0]\n",
    "    print(length1)\n",
    "    y = np.zeros((15939), dtype=int)\n",
    "    length2 = n2.shape[0]\n",
    "    print(length2)\n",
    "    y[0:length1] = 0\n",
    "    y[length1:length1+length2] = 1\n",
    "    \n",
    "    print(y)\n",
    "\n",
    "\n",
    "    # one hot encode\n",
    "    encoded = to_categorical(y)\n",
    "    print(encoded.shape)\n",
    "    return x,encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6963, 784)\n",
      "(8976, 784)\n",
      "8976\n",
      "6963\n",
      "[0 0 0 ... 1 1 1]\n",
      "(15939, 2)\n",
      "(15939, 784)\n",
      "(15939, 2)\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = get_data()\n",
    "\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "\n",
    "i = np.arange(15939)\n",
    "s = np.arange(i.shape[0])\n",
    "np.random.shuffle(s)\n",
    "\n",
    "batch_x = batch_x[s]\n",
    "batch_y = batch_y[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y[8:34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step, loss, accuracy, merged_summary_op = build_model(x, keep_prob,y_, output_node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [01:58<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.010482296\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [02:04<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 6.215855e-05\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [02:03<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 7.244242e-05\n",
      "Accuracy: 1.0\n",
      "training finished!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    tf.train.write_graph(sess.graph_def, 'out',\n",
    "        MODEL_NAME + '.pbtxt', True)\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter('logs/',\n",
    "        graph=tf.get_default_graph())\n",
    "\n",
    "    for _ in range(3):\n",
    "        for i in tqdm(range(0,batch_x.shape[0],50)):\n",
    "            x_data = batch_x[i:i+50]\n",
    "            y_data = batch_y[i:i+50]\n",
    "\n",
    "            _, c = sess.run([train_step, loss],\n",
    "                feed_dict={x: x_data, y_: y_data, keep_prob: 0.5})\n",
    "            \n",
    "        \n",
    "        \n",
    "        train_loss = loss.eval(feed_dict={\n",
    "            x: x_data, y_: y_data, keep_prob: 1.0})\n",
    "        print(\"Loss : \"+ str(train_loss))\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: x_data, y_: y_data, keep_prob: 1.0})\n",
    "        print(\"Accuracy: \"+ str(train_accuracy))\n",
    "\n",
    "    saver.save(sess, 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "print(\"training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freeze_graph.freeze_graph('out/' + MODEL_NAME + '.pbtxt', None, False,\n",
    "    'out/' + MODEL_NAME + '.chkp', output_node_name, \"save/restore_all\",\n",
    "    \"save/Const:0\", 'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "input_node_names = ['input']\n",
    "    \n",
    "output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "        input_graph_def, input_node_names, [output_node_name],\n",
    "        tf.float32.as_datatype_enum)\n",
    "\n",
    "with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "    f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
